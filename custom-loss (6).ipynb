{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tf = tensorflow\n",
    "from tensorflow import keras\n",
    "import math, random\n",
    "import numpy as np\n",
    "import time\n",
    "from Crypto.Util.number import getPrime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thinking\n",
    "\n",
    "Predict a 0 or 1, for a given bit, specified in the output hint. \n",
    "Example output formats: min(a,b); abs(a-b); (a+b)/2\n",
    "\n",
    "BTW: basic terminology problem - you should say semiprime instead of coprime... wtf\n",
    "\n",
    "### todo:\n",
    "- make it easier to run experiments and tune hyperparameters\n",
    "- come up with new hyperparams\n",
    "- start cataloguing which hyperparams improve performance; do a search by hand\n",
    "- create a curriculum for the mask_length\n",
    "- optimize for higher gpu usage; either by sticking the number generation into the gpu, speeding up prime number generation by caching prime numbers, using keras.fit[use_multiprocessing], doing the data generation in a C ext instead of python, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time per call(generate_problem)= 0.010872716910205782\n",
      "prime_a= 163 prime_b= 179 coprime= 29177\n",
      "example small problem: (array([ 1,  0,  0,  1,  1,  1,  1,  1,  1,  0,  0,  0,  1,  1,  1,  0,  0,\n",
      "        0,  0,  0,  1,  0,  0,  0, -2,  0, -1, -1,  0,  0,  0,  0],\n",
      "      dtype=int8), 0)\n"
     ]
    }
   ],
   "source": [
    "def generate_problem(prime_length=(140,140), mask_length=(1,280), samples=50, coprime_length=280, debug=False):\n",
    "    assert min(mask_length) > 0, 'must mask at least one bit which is the question'\n",
    "    assert coprime_length >= max(prime_length) * 2, 'coprime length needs to fit in 2 * max(prime_length) but doesnt'\n",
    "    base = 2\n",
    "\n",
    "    min_bits = min(prime_length)\n",
    "    max_bits = max(prime_length)\n",
    "    \n",
    "    prime_a_length = random.randint(min_bits, max_bits)\n",
    "    prime_b_length = random.randint(min_bits, max_bits)\n",
    "    # this is the slowest part of the code (5/6): verify by using random ints instead of primes\n",
    "    prime_a = getPrime(prime_a_length)\n",
    "#     prime_a = random.randint(1, 2**prime_a_length)\n",
    "    prime_b = getPrime(prime_b_length)\n",
    "#     prime_b = random.randint(1, 2**prime_b_length)\n",
    "    prime_a, prime_b = sorted([ prime_a, prime_b ])\n",
    "    \n",
    "#     shitty validation set lmao\n",
    "    if prime_a == 52163 and prime_b == 54577:\n",
    "        prime_a, prime_b = 43889, 50923\n",
    "    \n",
    "    coprime = prime_a * prime_b\n",
    "    \n",
    "    if debug:\n",
    "        print('prime_a=', prime_a, 'prime_b=', prime_b, 'coprime=', coprime)\n",
    "    \n",
    "    X = np.zeros(coprime_length * 2, dtype=np.int8)\n",
    "    coprime_array = X[:coprime_length]\n",
    "    coprime_index = 0\n",
    "    while coprime > 0:\n",
    "        coprime_array[coprime_index] = coprime % base\n",
    "        coprime = coprime // base\n",
    "        coprime_index += 1\n",
    "\n",
    "    factor_difference_array = X[coprime_length:]\n",
    "    factor_difference = abs(prime_a - prime_b)\n",
    "    factor_difference_index = 0\n",
    "    while factor_difference > 0:\n",
    "        factor_difference_array[factor_difference_index] = factor_difference % base\n",
    "        factor_difference = factor_difference // base\n",
    "        factor_difference_index += 1\n",
    "\n",
    "    for i in range(samples):\n",
    "        masked_X = X.copy()\n",
    "        masked_factor_difference_array = masked_X[coprime_length:]\n",
    "        masked_indices = np.random.choice(coprime_length, min(random.choice(range(min(mask_length), max(mask_length) + 1)), coprime_length), replace=False)\n",
    "        # for some reason the above is slow... :/\n",
    "        for index in masked_indices:\n",
    "            masked_factor_difference_array[index] = -1\n",
    "        prediction_index = random.choice(masked_indices)\n",
    "        masked_factor_difference_array[prediction_index] = -2\n",
    "        # for 0 or 1 as classes; if not using sigmoid as last layer...\n",
    "        # prediction = np.zeros(2, dtype=np.int8)\n",
    "        # prediction[factor_difference_array[prediction_index]] = 1\n",
    "        Y = factor_difference_array[prediction_index]\n",
    "        yield masked_X, Y\n",
    "\n",
    "N = 500\n",
    "timer_start = time.perf_counter()\n",
    "for i in range(N):\n",
    "    [i for i in generate_problem()]\n",
    "timer_end = time.perf_counter()\n",
    "print('time per call(generate_problem)=', (timer_end-timer_start) / N)\n",
    "\n",
    "print('example small problem:', [i for i in generate_problem(debug=True, prime_length=(8, 8), coprime_length=16, mask_length=(3,3), samples=1)][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prime_a= 179 prime_b= 251 coprime= 44929\n",
      "coprime: 44929 \n",
      "masked_factor_difference: [-2  0  0  1  0  0  1  0  0  0  0  0  0  0  0  0] \n",
      "Y: 0\n"
     ]
    }
   ],
   "source": [
    "def format_problem(X, Y):\n",
    "    coprime_length = X.shape[0] // 2\n",
    "    coprime_array = X[:coprime_length]\n",
    "    masked_factor_difference_array = X[coprime_length:]\n",
    "    coprime = 0\n",
    "    for power, bit in enumerate(coprime_array):\n",
    "        coprime += bit * (2 ** (power))\n",
    "    print('coprime:', coprime, '\\nmasked_factor_difference:', masked_factor_difference_array, '\\nY:', Y)\n",
    "\n",
    "format_problem(*[i for i in generate_problem(debug=True, prime_length=(8, 8), coprime_length=16, mask_length=(1, 1), samples=1)][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time per call(row_loader)= 0.005229469054378569\n"
     ]
    }
   ],
   "source": [
    "def row_loader(pg_args={}):\n",
    "    while True:\n",
    "        for X, Y in generate_problem(**pg_args):\n",
    "            yield X, Y\n",
    "    return\n",
    "\n",
    "N = 500\n",
    "timer_start = time.perf_counter()\n",
    "for i in range(N):\n",
    "    next(row_loader())\n",
    "timer_end = time.perf_counter()\n",
    "print('time per call(row_loader)=', (timer_end-timer_start) / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time per call(batch_loader)= 0.45302029312588277\n"
     ]
    }
   ],
   "source": [
    "def batch_loader(pg_args={}, batch_size=2000):\n",
    "    X, Y = next(generate_problem(**pg_args))\n",
    "    x_feature_size = X.shape[0]\n",
    "    y_feature_size = 1\n",
    "    while True:\n",
    "        batch_x = np.zeros((batch_size, x_feature_size), dtype=np.float32)\n",
    "        batch_y = np.zeros((batch_size, y_feature_size), dtype=np.float32)\n",
    "        for i, (x, y) in enumerate(row_loader(pg_args)):\n",
    "            if i >= batch_size:\n",
    "                break\n",
    "            batch_x[i,:] = x\n",
    "            batch_y[i,:] = y\n",
    "        yield batch_x, batch_y\n",
    "\n",
    "N = 10\n",
    "timer_start = time.perf_counter()\n",
    "for i in range(N):\n",
    "    next(batch_loader())\n",
    "#     next(batch_loader(pg_args={'mask_length': 120})) # masking most of the output is still wayyy too slow\n",
    "timer_end = time.perf_counter()\n",
    "print('time per call(batch_loader)=', (timer_end-timer_start) / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_44 (Dense)             (None, 1024)              68608     \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 2,168,833\n",
      "Trainable params: 2,168,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "pg_args = {\n",
    "    'prime_length': (2, 16),\n",
    "    'mask_length': (1, 33),\n",
    "    'samples': 100,\n",
    "    'coprime_length': 33,\n",
    "    'debug': False\n",
    "}\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(pg_args['coprime_length'] * 2, )))\n",
    "model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# TODO: make a custom metric which is accuracy over whole prime\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-af847b749f90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# ESPECIALLY: multiple threads should allow faster data generation, speedup in number of threads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# assuming bottlenecked on data generation... not sure what the evidence is though\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# model.fit(data_loader, epochs=1000, verbose=2, steps_per_epoch=1000, workers=0, use_multiprocessing=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    919\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    637\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# feel free to change these params\n",
    "# but be careful about adjusting other pg_args; some of them would change the shape of the network!\n",
    "\n",
    "# MEMORY USAGE = (coprime_length * 2 + 1) * batch_size + 1\n",
    "# (in bytes)\n",
    "\n",
    "batch_size = 2000\n",
    "pg_args['samples'] = 20\n",
    "pg_args['mask_length'] = (1, pg_args['coprime_length'])\n",
    "\n",
    "data_loader = batch_loader(pg_args=pg_args, batch_size=batch_size)\n",
    "\n",
    "# TODO: experiment with use_multiprocessing, workers, tf.keras.Sequence, tf.data, etc\n",
    "# ESPECIALLY: multiple threads should allow faster data generation, speedup in number of threads\n",
    "# assuming bottlenecked on data generation... not sure what the evidence is though\n",
    "model.fit(data_loader, epochs=1000, verbose=2, steps_per_epoch=1000)\n",
    "# model.fit(data_loader, epochs=1000, verbose=2, steps_per_epoch=1000, workers=0, use_multiprocessing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 5s - loss: 0.0170 - accuracy: 0.9782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.016974860802292824, 0.97816002368927]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(data_loader, verbose=2, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coprime: 177.0 \n",
      "masked_factor_difference: [ 0.  0. -1. -1.  1.  1.  0. -1. -2.  0.  0.  0.  0.  0.  0. -1.  0.] \n",
      "Y: [0.]\n",
      "\n",
      "prediction: [2.6849452e-06]\n"
     ]
    }
   ],
   "source": [
    "def example_problem(model, data_loader):\n",
    "    batch_input, batch_output = next(data_loader)\n",
    "    batch_prediction = model.predict(batch_input)\n",
    "\n",
    "    example_input = batch_input[0]\n",
    "    example_output = batch_output[0]\n",
    "    example_prediction = batch_prediction[0]\n",
    "\n",
    "    format_problem(example_input, example_output)\n",
    "    print('\\nprediction:', example_prediction)\n",
    "    return\n",
    "\n",
    "example_problem(model, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primes_from_coprime_difference(coprime, factor_difference):\n",
    "    average_of_factors = ((factor_difference * factor_difference + 4 * coprime) ** 0.5) / 2.0\n",
    "    \n",
    "    prime_a = average_of_factors + factor_difference / 2.0\n",
    "    prime_b = average_of_factors - factor_difference / 2.0\n",
    "    \n",
    "    return prime_a, prime_b\n",
    "\n",
    "def attempt_solution(coprime, model, debug=False):\n",
    "    base = 2  # previously wasnt hardcoded but now hardcoding to keep things simple\n",
    "    X_len = model.layers[0].input_shape[1]\n",
    "    X_len_half = X_len//2\n",
    "    X = np.zeros(X_len, dtype=np.int8)\n",
    "    \n",
    "    coprime_array = X[:X_len_half]\n",
    "    coprime_index = 0\n",
    "    coprime_residue = coprime\n",
    "    while coprime_residue > 0:\n",
    "        coprime_array[coprime_index] = coprime_residue % base\n",
    "        coprime_residue = coprime_residue // base\n",
    "        coprime_index += 1\n",
    "    \n",
    "    factor_difference = X[X_len_half:]\n",
    "    for i in range(X_len_half):\n",
    "        factor_difference[i] = -1\n",
    "    \n",
    "    # in the future try iterating in the last 5-10 unknown bits, since it's likely faster\n",
    "    # also consider sampling based off probabilities instead of going with argmax\n",
    "    while list(factor_difference).count(-1) > 0:\n",
    "        if debug:\n",
    "            print('factor_difference=', factor_difference)\n",
    "        highest_confidence = -1.0\n",
    "        highest_confidence_index = None\n",
    "        highest_confidence_value = None\n",
    "        for i in range(X_len_half):\n",
    "            if factor_difference[i] == -1:\n",
    "                factor_difference[i] = -2\n",
    "                Y = model.predict(np.array([X]))\n",
    "#                 if debug: # crazy debugging\n",
    "#                     print(X, Y)\n",
    "                confidence = abs(Y - 0.5) * 2\n",
    "                if confidence > highest_confidence:\n",
    "                    highest_confidence = confidence\n",
    "                    highest_confidence_index = i\n",
    "                    highest_confidence_value = int(Y > 0.5)\n",
    "                factor_difference[i] = -1\n",
    "        if debug:\n",
    "            print('highest_confidence=', highest_confidence)\n",
    "        factor_difference[highest_confidence_index] = highest_confidence_value\n",
    "    \n",
    "    if debug:\n",
    "        print('factor_difference=', factor_difference)\n",
    "\n",
    "    factor_difference_array = factor_difference\n",
    "    factor_difference = 0\n",
    "    for power, bit in enumerate(factor_difference_array):\n",
    "        factor_difference += bit * (2 ** power)\n",
    "    \n",
    "    if debug:\n",
    "        print('factor_difference[int]=', factor_difference)\n",
    "    \n",
    "    prime_a, prime_b = primes_from_coprime_difference(coprime, factor_difference)\n",
    "    \n",
    "    return prime_a, prime_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factor_difference= [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [ 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [ 0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [ 0  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [ 0  0  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [ 0  0  0  0 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [ 0  0  0  0  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [ 0  0  0  0  0  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [ 0  0  0  0  0  0  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [ 0  0  0  0  0  0  0  0  0 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [ 0  0  0  0  0  0  0  0  0  0 -1 -1 -1 -1 -1 -1 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [ 0  0  0  0  0  0  0  0  0  0  0 -1 -1 -1 -1 -1 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [ 0  0  0  0  0  0  0  0  0  0  0  0 -1 -1 -1 -1 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [ 0  0  0  0  0  0  0  0  0  0  0  0  0 -1 -1 -1 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1 -1 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1]\n",
      "highest_confidence= [[1.]]\n",
      "factor_difference= [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "factor_difference[int]= 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.744562646538029, 5.744562646538029)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attempt_solution(52163 * 54577, model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prime_a= 43889 prime_b= 50923 coprime= 2234959547\n",
      "coprime: 2234959547 \n",
      "masked_factor_difference: [ 0  1  0  1  1  1  1  0  1  1  0  1  1  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 -2  0] \n",
      "Y: 0\n"
     ]
    }
   ],
   "source": [
    "format_problem(*[i for i in generate_problem(debug=True, prime_length=(16, 16), coprime_length=32, mask_length=(1, 1), samples=1)][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
